{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ekagr\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ekagr\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ekagr\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv('twitter_training.csv')\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID      Feature Sentiment  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "                                                Text  \n",
      "0  I am coming to the borders and I will kill you...  \n",
      "1  im getting on borderlands and i will kill you ...  \n",
      "2  im coming on borderlands and i will murder you...  \n",
      "3  im getting on borderlands 2 and i will murder ...  \n",
      "4  im getting into borderlands and i can murder y...  \n"
     ]
    }
   ],
   "source": [
    "dataset.columns = ['ID', 'Feature', 'Sentiment', 'Text']\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>So I spent a couple of hours doing something f...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So I spent a few hours doing something for fun...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So I spent a few hours making something for fu...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010 So I spent a few hours making something f...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  I am coming to the borders and I will kill you...  Positive\n",
       "1  im getting on borderlands and i will kill you ...  Positive\n",
       "2  im coming on borderlands and i will murder you...  Positive\n",
       "3  im getting on borderlands 2 and i will murder ...  Positive\n",
       "4  im getting into borderlands and i can murder y...  Positive\n",
       "5  So I spent a few hours making something for fu...  Positive\n",
       "6  So I spent a couple of hours doing something f...  Positive\n",
       "7  So I spent a few hours doing something for fun...  Positive\n",
       "8  So I spent a few hours making something for fu...  Positive\n",
       "9  2010 So I spent a few hours making something f...  Positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[['Text', 'Sentiment']]\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset=['Text', 'Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Negative      22358\n",
      "Positive      20654\n",
      "Neutral       18108\n",
      "Irrelevant    12875\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Positive': 2, 'Neutral': 1, 'Negative': 0, 'Irrelevant': 3}\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "0    22358\n",
      "2    20654\n",
      "1    18108\n",
      "3    12875\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2 - Positive\n",
    "# 1 - Neutral\n",
    "# 0 - Negative\n",
    "# 3 - Irrelevant\n",
    "\n",
    "print(dataset['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)    # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)    # Remove hashtags\n",
    "    text = re.sub(r\"\\d+\", \"\", text)     # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text) # Remove punctuation\n",
    "    text = text.lower().strip()         # Convert to lowercase and strip whitespace\n",
    "    return text\n",
    "\n",
    "dataset['Text'] = dataset['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenized_data = tokenizer(list(dataset['Text']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "import torch\n",
    "labels = torch.tensor(dataset['Sentiment'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    tokenized_data['input_ids'], labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, precision_score, recall_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='weighted'),\n",
    "        'precision': precision_score(labels, predictions, average='weighted'),\n",
    "        'recall': recall_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "# Convert the datasets to the appropriate format\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": train_texts, \"labels\": train_labels})\n",
    "eval_dataset = Dataset.from_dict({\"input_ids\": test_texts, \"labels\": test_labels})\n",
    "\n",
    "\n",
    "eval_dataset_list = list(eval_dataset)  # Convert to a list\n",
    "sampled_eval_dataset = random.sample(eval_dataset_list, 4500)  # Take 4500 random samples\n",
    "\n",
    "# Convert back to a Dataset object\n",
    "eval_dataset = Dataset.from_list(sampled_eval_dataset)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directory to save model and checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate only at the end of each epoch\n",
    "    save_strategy=\"steps\",  # Save checkpoints regularly\n",
    "    save_total_limit=3,  # Number of maximum checkpoints to save\n",
    "    save_steps=500,  # Save checkpoints every 500 steps\n",
    "    eval_steps=500,  # Evaluate model every 500 steps\n",
    "    logging_dir='./logs',  # Directory for logs\n",
    "    logging_steps=100,  # Log every 25 steps\n",
    "    learning_rate=2e-5,  # Use a smaller learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,  # Reduce batch size\n",
    "    per_device_eval_batch_size=8,  # Reduce batch size for evaluation\n",
    "    num_train_epochs=3,  # Train for a smaller number of epochs\n",
    "    weight_decay=0.01,  # Regularization to avoid overfitting\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    no_cuda=True,  # Do not use CUDA (since you’re on a CPU)\n",
    "    fp16=False,  # Disable mixed precision training\n",
    "    load_best_model_at_end=True,  # Save the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use evaluation loss to track the best model\n",
    "    greater_is_better=False,  # We want to minimize the loss\n",
    "    push_to_hub=False,  # If you don't plan to push to the HuggingFace hub\n",
    "    report_to=[\"tensorboard\"], \n",
    "    log_level=\"info\",  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./results\\checkpoint-8400.\n",
      "c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "***** Running training *****\n",
      "  Num examples = 59,196\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 11,100\n",
      "  Number of trainable parameters = 66,956,548\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 500 (from args) != 200 (from trainer_state.json)\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 2\n",
      "  Continuing training from global step 8400\n",
      "  Will skip the first 2 epochs then the first 2000 batches in the first epoch.\n",
      "  0%|          | 0/11100 [00:00<?, ?it/s]c:\\Users\\ekagr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      " 77%|███████▋  | 8500/11100 [08:42<3:14:57,  4.50s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2737, 'grad_norm': 13.347362518310547, 'learning_rate': 4.684684684684685e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 77%|███████▋  | 8500/11100 [14:38<3:14:57,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47807833552360535, 'eval_accuracy': 0.8448888888888889, 'eval_f1': 0.8441686819933549, 'eval_precision': 0.8450396337253958, 'eval_recall': 0.8448888888888889, 'eval_runtime': 355.6528, 'eval_samples_per_second': 12.653, 'eval_steps_per_second': 1.583, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 8600/11100 [22:24<3:16:37,  4.72s/it]  Saving model checkpoint to ./results\\checkpoint-8600\n",
      "Configuration saved in ./results\\checkpoint-8600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2654, 'grad_norm': 26.430622100830078, 'learning_rate': 4.504504504504505e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-8600\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-7000] due to args.save_total_limit\n",
      " 78%|███████▊  | 8700/11100 [30:21<3:22:26,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3026, 'grad_norm': 45.32455825805664, 'learning_rate': 4.324324324324325e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 8800/11100 [38:21<2:47:03,  4.36s/it]Saving model checkpoint to ./results\\checkpoint-8800\n",
      "Configuration saved in ./results\\checkpoint-8800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2931, 'grad_norm': 19.571107864379883, 'learning_rate': 4.1441441441441446e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-8800\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-8200] due to args.save_total_limit\n",
      " 80%|████████  | 8900/11100 [47:36<3:22:01,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3112, 'grad_norm': 18.67558479309082, 'learning_rate': 3.9639639639639645e-06, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 9000/11100 [56:44<3:10:31,  5.44s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2873, 'grad_norm': 11.543493270874023, 'learning_rate': 3.7837837837837844e-06, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 81%|████████  | 9000/11100 [1:03:36<3:10:31,  5.44s/it]Saving model checkpoint to ./results\\checkpoint-9000\n",
      "Configuration saved in ./results\\checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47529101371765137, 'eval_accuracy': 0.8497777777777777, 'eval_f1': 0.8491364100560604, 'eval_precision': 0.8535463937099487, 'eval_recall': 0.8497777777777777, 'eval_runtime': 411.7965, 'eval_samples_per_second': 10.928, 'eval_steps_per_second': 1.367, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-9000\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-8400] due to args.save_total_limit\n",
      " 82%|████████▏ | 9100/11100 [1:12:40<3:10:47,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2807, 'grad_norm': 14.370914459228516, 'learning_rate': 3.603603603603604e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 9200/11100 [1:19:59<2:14:49,  4.26s/it]Saving model checkpoint to ./results\\checkpoint-9200\n",
      "Configuration saved in ./results\\checkpoint-9200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2815, 'grad_norm': 12.268699645996094, 'learning_rate': 3.423423423423424e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-9200\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-8600] due to args.save_total_limit\n",
      " 84%|████████▍ | 9300/11100 [1:28:54<2:51:47,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2847, 'grad_norm': 19.133731842041016, 'learning_rate': 3.2432432432432437e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 9400/11100 [1:38:40<2:55:45,  6.20s/it]Saving model checkpoint to ./results\\checkpoint-9400\n",
      "Configuration saved in ./results\\checkpoint-9400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2487, 'grad_norm': 53.0, 'learning_rate': 3.063063063063063e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-9400\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-8800] due to args.save_total_limit\n",
      " 86%|████████▌ | 9500/11100 [1:48:31<3:07:42,  7.04s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2955, 'grad_norm': 38.42658615112305, 'learning_rate': 2.882882882882883e-06, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 86%|████████▌ | 9500/11100 [1:55:49<3:07:42,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4561731815338135, 'eval_accuracy': 0.8566666666666667, 'eval_f1': 0.8565326867699419, 'eval_precision': 0.8590963040618808, 'eval_recall': 0.8566666666666667, 'eval_runtime': 437.4532, 'eval_samples_per_second': 10.287, 'eval_steps_per_second': 1.287, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 9600/11100 [2:04:04<2:21:05,  5.64s/it]  Saving model checkpoint to ./results\\checkpoint-9600\n",
      "Configuration saved in ./results\\checkpoint-9600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2474, 'grad_norm': 13.453513145446777, 'learning_rate': 2.702702702702703e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-9600\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-9000] due to args.save_total_limit\n",
      " 87%|████████▋ | 9700/11100 [2:12:21<1:37:42,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2842, 'grad_norm': 15.448568344116211, 'learning_rate': 2.5225225225225225e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 9800/11100 [2:20:12<1:30:23,  4.17s/it]Saving model checkpoint to ./results\\checkpoint-9800\n",
      "Configuration saved in ./results\\checkpoint-9800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2934, 'grad_norm': 8.081794738769531, 'learning_rate': 2.3423423423423424e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-9800\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-9200] due to args.save_total_limit\n",
      " 89%|████████▉ | 9900/11100 [2:27:59<1:47:11,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2617, 'grad_norm': 17.271671295166016, 'learning_rate': 2.1621621621621623e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 10000/11100 [2:37:13<1:41:47,  5.55s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2456, 'grad_norm': 54.971885681152344, 'learning_rate': 1.9819819819819822e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 90%|█████████ | 10000/11100 [2:44:07<1:41:47,  5.55s/it]Saving model checkpoint to ./results\\checkpoint-10000\n",
      "Configuration saved in ./results\\checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45150473713874817, 'eval_accuracy': 0.8595555555555555, 'eval_f1': 0.8590442526368443, 'eval_precision': 0.8604888282425538, 'eval_recall': 0.8595555555555555, 'eval_runtime': 413.0642, 'eval_samples_per_second': 10.894, 'eval_steps_per_second': 1.363, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-10000\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-9400] due to args.save_total_limit\n",
      " 91%|█████████ | 10100/11100 [2:52:55<1:31:38,  5.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3148, 'grad_norm': 15.49498462677002, 'learning_rate': 1.801801801801802e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 10200/11100 [3:02:04<1:22:49,  5.52s/it]Saving model checkpoint to ./results\\checkpoint-10200\n",
      "Configuration saved in ./results\\checkpoint-10200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2835, 'grad_norm': 51.479007720947266, 'learning_rate': 1.6216216216216219e-06, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-10200\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-9600] due to args.save_total_limit\n",
      " 93%|█████████▎| 10300/11100 [3:10:50<1:13:33,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2895, 'grad_norm': 31.433181762695312, 'learning_rate': 1.4414414414414416e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 10400/11100 [3:19:38<1:01:45,  5.29s/it]Saving model checkpoint to ./results\\checkpoint-10400\n",
      "Configuration saved in ./results\\checkpoint-10400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2569, 'grad_norm': 52.24767303466797, 'learning_rate': 1.2612612612612613e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-10400\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-9800] due to args.save_total_limit\n",
      " 95%|█████████▍| 10500/11100 [3:26:44<44:58,  4.50s/it]  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2527, 'grad_norm': 17.806425094604492, 'learning_rate': 1.0810810810810812e-06, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 95%|█████████▍| 10500/11100 [3:32:08<44:58,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44270598888397217, 'eval_accuracy': 0.8606666666666667, 'eval_f1': 0.8601028563574963, 'eval_precision': 0.8608742732602637, 'eval_recall': 0.8606666666666667, 'eval_runtime': 324.2004, 'eval_samples_per_second': 13.88, 'eval_steps_per_second': 1.737, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 10600/11100 [3:39:10<34:13,  4.11s/it]    Saving model checkpoint to ./results\\checkpoint-10600\n",
      "Configuration saved in ./results\\checkpoint-10600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27, 'grad_norm': 37.82451629638672, 'learning_rate': 9.00900900900901e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-10600\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-10000] due to args.save_total_limit\n",
      " 96%|█████████▋| 10700/11100 [3:46:14<29:59,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2745, 'grad_norm': 42.30682373046875, 'learning_rate': 7.207207207207208e-07, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 10800/11100 [3:53:27<20:29,  4.10s/it]Saving model checkpoint to ./results\\checkpoint-10800\n",
      "Configuration saved in ./results\\checkpoint-10800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2782, 'grad_norm': 15.56509017944336, 'learning_rate': 5.405405405405406e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-10800\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-10200] due to args.save_total_limit\n",
      " 98%|█████████▊| 10900/11100 [4:00:26<14:01,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2518, 'grad_norm': 22.378799438476562, 'learning_rate': 3.603603603603604e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11000/11100 [4:07:56<06:54,  4.14s/it]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4500\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2488, 'grad_norm': 81.6471176147461, 'learning_rate': 1.801801801801802e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 99%|█████████▉| 11000/11100 [4:14:02<06:54,  4.14s/it]Saving model checkpoint to ./results\\checkpoint-11000\n",
      "Configuration saved in ./results\\checkpoint-11000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43545860052108765, 'eval_accuracy': 0.8606666666666667, 'eval_f1': 0.8603581250384227, 'eval_precision': 0.8613937501470381, 'eval_recall': 0.8606666666666667, 'eval_runtime': 366.4612, 'eval_samples_per_second': 12.28, 'eval_steps_per_second': 1.536, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-11000\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-10400] due to args.save_total_limit\n",
      "100%|██████████| 11100/11100 [4:22:07<00:00,  3.81s/it]   Saving model checkpoint to ./results\\checkpoint-11100\n",
      "Configuration saved in ./results\\checkpoint-11100\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2995, 'grad_norm': 35.4686164855957, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-11100\\model.safetensors\n",
      "Deleting older checkpoint [results\\checkpoint-10600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-11000 (score: 0.43545860052108765).\n",
      "100%|██████████| 11100/11100 [4:22:10<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15730.4082, 'train_samples_per_second': 11.289, 'train_steps_per_second': 0.706, 'train_loss': 0.06736033242028039, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'setting', 'settings': {'python.tensorboard.logDirectory': './logs'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Assuming `training_args` has been defined and training has started before\n",
    "last_checkpoint = None\n",
    "checkpoints = [d for d in os.listdir(training_args.output_dir) if 'checkpoint' in d]\n",
    "if checkpoints:\n",
    "    last_checkpoint = os.path.join(training_args.output_dir, sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1])\n",
    "\n",
    "\n",
    "# # sess.graph contains the graph definition; that enables the Graph Visualizer.\n",
    "# with SummaryWriter(log_dir=training_args.logging_dir) as sw:\n",
    "#     trainer.train(resume_from_checkpoint=last_checkpoint if last_checkpoint else None)\n",
    "\n",
    "# Now, resume from the last checkpoint\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint if last_checkpoint else None)\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "writer = SummaryWriter(log_dir=training_args.logging_dir)\n",
    "\n",
    "# Example: Log a custom scalar\n",
    "writer.add_scalar(\"custom_metric\", 0.95, global_step=1)\n",
    "\n",
    "# Close the writer after training\n",
    "writer.close()\n",
    "\n",
    "# TensorBoard log directory setting\n",
    "{\n",
    "  \"type\": \"setting\",\n",
    "  \"settings\": {\n",
    "    \"python.tensorboard.logDirectory\": \"./logs\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14799\n",
      "  Batch size = 8\n",
      "100%|██████████| 1850/1850 [19:05<00:00,  1.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4403340518474579,\n",
       " 'eval_accuracy': 0.8553280627069396,\n",
       " 'eval_f1': 0.8551453091158301,\n",
       " 'eval_precision': 0.8557576722681051,\n",
       " 'eval_recall': 0.8553280627069396,\n",
       " 'eval_runtime': 1146.4615,\n",
       " 'eval_samples_per_second': 12.908,\n",
       " 'eval_steps_per_second': 1.614,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "finaleval = Dataset.from_list(eval_dataset_list) \n",
    "\n",
    "\n",
    "trainer.evaluate(eval_dataset=finaleval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./sentiment_model\\config.json\n",
      "Model weights saved in ./sentiment_model\\model.safetensors\n",
      "tokenizer config file saved in ./sentiment_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./sentiment_model\\\\tokenizer_config.json',\n",
       " './sentiment_model\\\\special_tokens_map.json',\n",
       " './sentiment_model\\\\vocab.txt',\n",
       " './sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./sentiment_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the current notebook...\n"
     ]
    },
    {
     "data": {
      "application/javascript": "IPython.notebook.save_checkpoint();",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "# Save the current notebook\n",
    "def save_current_notebook():\n",
    "    print(\"Saving the current notebook...\")\n",
    "    display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "\n",
    "save_current_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WARNING WILL SHUTDOWN YOUR COMPUTER\n",
    "\n",
    "# import ctypes\n",
    "# user32 = ctypes.WinDLL('user32')\n",
    "# user32.ExitWindowsEx(0x00000008, 0x00000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
