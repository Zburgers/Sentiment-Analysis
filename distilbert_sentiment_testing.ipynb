{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"./sentiment_model\"\n",
    "TEST_DATA_PATH = \"./twitter_training.csv\"\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['2401', 'Borderlands', 'Positive', 'im getting on borderlands and i will murder you all ,']\n",
      "First few rows:\n",
      "    2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and inspect the CSV file\n",
    "file_path = \"./twitter_training.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset columns:\", df.columns.tolist())\n",
    "    print(\"First few rows:\\n\", df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "try:\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data loaded and processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the test data\n",
    "def load_test_data(file_path, sample_size):\n",
    "    print(\"Loading test data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Rename columns to match required format\n",
    "    df = df.rename(columns={\n",
    "        df.columns[3]: \"text\",  # Assuming the 4th column is the text\n",
    "        df.columns[2]: \"label\"  # Assuming the 3rd column is the label\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column names exist\n",
    "    required_columns = [\"text\", \"label\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Dataset is missing required columns: {required_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in critical columns\n",
    "    df = df.dropna(subset=required_columns)\n",
    "    \n",
    "    # Map labels to integers\n",
    "    label_mapping = {\n",
    "        \"Positive\": 2,  # Most frequent in training\n",
    "        \"Neutral\": 1,\n",
    "        \"Negative\": 0,\n",
    "        \"Irrelevant\": 3  # Least frequent in training\n",
    "    }\n",
    "    df[\"label\"] = df[\"label\"].map(label_mapping)\n",
    "    \n",
    "    # Validate label mapping\n",
    "    if df[\"label\"].isnull().any():\n",
    "        raise ValueError(\"Found unmapped labels in the dataset.\")\n",
    "    \n",
    "    df_sampled = df.head(sample_size)\n",
    "    \n",
    "    print(\"Test data loaded and processed successfully.\")\n",
    "    return df_sampled\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    TEST_DATA_PATH = \"./twitter_training.csv\"\n",
    "    test_data = load_test_data(TEST_DATA_PATH, sample_size=1000)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader\n",
    "test_dataset = SentimentDataset(test_data, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    print(\"Evaluating model...\")\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Evaluation complete.\")\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluation complete.\n",
      "Generating performance report...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.79      0.81      0.80       195\n",
      "     Neutral       0.69      0.71      0.70       279\n",
      "    Negative       0.83      0.83      0.83       424\n",
      "  Irrelevant       0.69      0.64      0.66       102\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.75      0.75      0.75      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      "\n",
      "Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate predictions and performance metrics\n",
    "try:\n",
    "    true_labels, predictions = evaluate_model(model, test_loader)\n",
    "    print(\"Generating performance report...\")\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"]))\n",
    "    print(f\"Accuracy: {accuracy_score(true_labels, predictions):.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
